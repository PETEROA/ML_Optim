{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PETEROA/ML_Optim/blob/main/EdgeML_Optim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3sRSIOhn6D2"
      },
      "source": [
        "Edge ML Optimisation Demo\n",
        "\n",
        "Author: Peter Agida\n",
        "\n",
        "Objective: Compress image classification model for mobile deployment\n",
        "\n",
        "Target: 5-10x speedup, <2% accuracy loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LsSw59zynZaX",
        "outputId": "d9bad4e6-a2ea-4fa3-ba79-0c170e1b1120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.20.0 onnxruntime-1.23.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from ptflops) (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.3)\n",
            "Downloading ptflops-0.7.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.5\n",
            "Collecting torch-pruning\n",
            "  Downloading torch_pruning-1.6.1-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from torch-pruning) (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-pruning) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->torch-pruning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->torch-pruning) (3.0.3)\n",
            "Downloading torch_pruning-1.6.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m70.2/70.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-pruning\n",
            "Successfully installed torch-pruning-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install onnx onnxruntime\n",
        "!pip install tensorflow\n",
        "!pip install plotly\n",
        "!pip install ptflops\n",
        "!pip install torch-pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTwnloUboVH2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "from ptflops import get_model_complexity_info\n",
        "import onnx\n",
        "import onnxruntime\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X81DzRzeoqxW",
        "outputId": "bd2154a5-f7e0-4e8a-cdcd-f16a8b92e40d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete!\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjZxhwtio8HR",
        "outputId": "9c9f1888-b3f0-428a-e234-f0b1b242410c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:05<00:00, 33.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset loaded: 50000 train, 10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Dataset Preparation\n",
        "print(\"Preparing dataset...\")\n",
        "\n",
        "# Use CIFAR-10 for quick demo (can scale to ImageNet later)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\" Dataset loaded: {len(train_dataset)} train, {len(test_dataset)} test samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4nK-ttX0qse2",
        "outputId": "ba0c4a57-e652-4cac-b58a-3c64261c93bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher model created\n",
            "   Device: cpu\n"
          ]
        }
      ],
      "source": [
        "#Baseline Model (Teacher)\n",
        "class TeacherModel(nn.Module):\n",
        "    \"\"\"Large teacher model - ResNet18 modified for CIFAR-10\"\"\"\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
        "        # Modify first conv for CIFAR-10 (32x32 images)\n",
        "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.model.maxpool = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize teacher\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "teacher = TeacherModel().to(device)\n",
        "\n",
        "print(f\"Teacher model created\")\n",
        "print(f\"   Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R8EPaPUgrHrA",
        "outputId": "bc7af45a-9760-4c52-8b0c-1d600a456382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Baseline Model Stats:\n",
            "   Parameters: 11,173,962\n",
            "   FLOPs: 1,115,564,052\n",
            "   Size: 42.66 MB\n"
          ]
        }
      ],
      "source": [
        "# Model Complexity Analysis\n",
        "def analyze_model(model, input_size=(3, 32, 32)):\n",
        "    \"\"\"Analyze model size, params, and FLOPs\"\"\"\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Get FLOPs\n",
        "    macs, params = get_model_complexity_info(\n",
        "        model, input_size, as_strings=False,\n",
        "        print_per_layer_stat=False, verbose=False\n",
        "    )\n",
        "    flops = 2 * macs  # MACs to FLOPs approximation\n",
        "\n",
        "    # Model size\n",
        "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
        "    size_mb = (param_size + buffer_size) / (1024**2)\n",
        "\n",
        "    return {\n",
        "        'params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'flops': flops,\n",
        "        'size_mb': size_mb\n",
        "    }\n",
        "\n",
        "baseline_stats = analyze_model(teacher)\n",
        "print(f\"\\n Baseline Model Stats:\")\n",
        "print(f\"   Parameters: {baseline_stats['params']:,}\")\n",
        "print(f\"   FLOPs: {baseline_stats['flops']:,}\")\n",
        "print(f\"   Size: {baseline_stats['size_mb']:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsxsL5PmrcAa"
      },
      "source": [
        "Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCqPEC21rVi0",
        "outputId": "548fec3c-5b49-4b5a-f4b9-4ee2382b2187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training utilities defined\n"
          ]
        }
      ],
      "source": [
        "# Training  and evaluation functions\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device, epochs=10):\n",
        "    \"\"\"Standard training loop\"\"\"\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], '\n",
        "                      f'Loss: {running_loss/100:.3f}, Acc: {100.*correct/total:.2f}%')\n",
        "                running_loss = 0.0\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate model accuracy\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def benchmark_latency(model, input_size=(1, 3, 32, 32), device='cpu', num_runs=100):\n",
        "    \"\"\"Measure inference latency\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = model(dummy_input)\n",
        "\n",
        "    # Benchmark\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        _ = model(dummy_input)\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    avg_latency_ms = (end - start) / num_runs * 1000\n",
        "    return avg_latency_ms\n",
        "\n",
        "print(\" Training utilities defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CtfnjHHrv6O"
      },
      "source": [
        "Train Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfXmhd_yrp1C",
        "outputId": "d349a874-f45b-491f-8d4a-730349dfb688"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéì Training teacher model...\n",
            "Epoch [1/10], Step [100/391], Loss: 2.763, Acc: 15.17%\n",
            "Epoch [1/10], Step [200/391], Loss: 1.939, Acc: 21.03%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\nüéì Training teacher model...\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(teacher.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# Train teacher (reduce epochs for demo - use 20+ for production)\n",
        "TEACHER_EPOCHS = 10  # Set to 20-50 for better results\n",
        "train_model(teacher, train_loader, criterion, optimizer, device, epochs=TEACHER_EPOCHS)\n",
        "\n",
        "# Evaluate teacher\n",
        "teacher_accuracy = evaluate_model(teacher, test_loader, device)\n",
        "teacher_latency = benchmark_latency(teacher, device=str(device))\n",
        "\n",
        "print(f\"\\nTeacher Model Results:\")\n",
        "print(f\"   Accuracy: {teacher_accuracy:.2f}%\")\n",
        "print(f\"   Latency: {teacher_latency:.2f} ms\")\n",
        "print(f\"   Size: {baseline_stats['size_mb']:.2f} MB\")\n",
        "\n",
        "# Save teacher model\n",
        "torch.save(teacher.state_dict(), 'teacher_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlK9A1mmXSE4"
      },
      "source": [
        "Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URxCcFDVXFAr"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STUDENT MODEL (Lightweight)\n",
        "# ============================================================================\n",
        "class StudentModel(nn.Module):\n",
        "    \"\"\"Compact student model - 10x smaller\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(StudentModel, self).__init__()\n",
        "\n",
        "        # Simplified architecture\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),  # 16x16\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),  # 8x8\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "student = StudentModel().to(device)\n",
        "student_stats = analyze_model(student)\n",
        "\n",
        "print(f\"\\n Student Model Stats:\")\n",
        "print(f\"   Parameters: {student_stats['params']:,} ({baseline_stats['params']/student_stats['params']:.1f}x smaller)\")\n",
        "print(f\"   FLOPs: {student_stats['flops']:,} ({baseline_stats['flops']/student_stats['flops']:.1f}x fewer)\")\n",
        "print(f\"   Size: {student_stats['size_mb']:.2f} MB ({baseline_stats['size_mb']/student_stats['size_mb']:.1f}x smaller)\")\n",
        "\n",
        "# ============================================================================\n",
        "# KNOWLEDGE DISTILLATION TRAINING\n",
        "# ============================================================================\n",
        "class DistillationLoss(nn.Module):\n",
        "    \"\"\"Combines hard labels and soft labels from teacher\"\"\"\n",
        "    def __init__(self, temperature=4.0, alpha=0.7):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Distillation loss (soft targets)\n",
        "        distill_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits / self.temperature, dim=1),\n",
        "            F.softmax(teacher_logits / self.temperature, dim=1),\n",
        "            reduction='batchmean'\n",
        "        ) * (self.temperature ** 2)\n",
        "\n",
        "        # Student loss (hard targets)\n",
        "        student_loss = self.criterion(student_logits, labels)\n",
        "\n",
        "        # Combined loss\n",
        "        return self.alpha * distill_loss + (1 - self.alpha) * student_loss\n",
        "\n",
        "def train_with_distillation(student, teacher, train_loader, device, epochs=20):\n",
        "    \"\"\"Train student using knowledge distillation\"\"\"\n",
        "    teacher.eval()  # Teacher in eval mode\n",
        "    student.train()\n",
        "\n",
        "    criterion = DistillationLoss(temperature=4.0, alpha=0.7)\n",
        "    optimizer = torch.optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher predictions\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            # Get student predictions\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            # Compute distillation loss\n",
        "            loss = criterion(student_logits, teacher_logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = student_logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], '\n",
        "                      f'Loss: {running_loss/100:.3f}, Acc: {100.*correct/total:.2f}%')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "print(\"\\n Training student with knowledge distillation...\")\n",
        "DISTILL_EPOCHS = 15  # Set to 30-50 for production\n",
        "train_with_distillation(student, teacher, train_loader, device, epochs=DISTILL_EPOCHS)\n",
        "\n",
        "# Evaluate distilled student\n",
        "student_accuracy = evaluate_model(student, test_loader, device)\n",
        "student_latency = benchmark_latency(student, device=str(device))\n",
        "\n",
        "print(f\"\\n Distilled Student Results:\")\n",
        "print(f\"   Accuracy: {student_accuracy:.2f}% (vs Teacher: {teacher_accuracy:.2f}%)\")\n",
        "print(f\"   Accuracy drop: {teacher_accuracy - student_accuracy:.2f}%\")\n",
        "print(f\"   Latency: {student_latency:.2f} ms ({teacher_latency/student_latency:.1f}x faster)\")\n",
        "print(f\"   Size: {student_stats['size_mb']:.2f} MB ({baseline_stats['size_mb']/student_stats['size_mb']:.1f}x smaller)\")\n",
        "\n",
        "torch.save(student.state_dict(), 'student_distilled.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtnGVJeHXipf"
      },
      "source": [
        "Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeWQNjyeXlz_"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# POST-TRAINING QUANTIZATION\n",
        "# ============================================================================\n",
        "print(\"\\n Applying quantization...\")\n",
        "\n",
        "# Dynamic quantization (easiest, good for CPU)\n",
        "student_quantized = torch.quantization.quantize_dynamic(\n",
        "    student.cpu(), {nn.Linear, nn.Conv2d}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Evaluate quantized model\n",
        "student_quant_accuracy = evaluate_model(student_quantized, test_loader, 'cpu')\n",
        "student_quant_latency = benchmark_latency(student_quantized, device='cpu')\n",
        "\n",
        "# Calculate quantized model size\n",
        "torch.save(student_quantized.state_dict(), 'student_quantized.pth')\n",
        "import os\n",
        "quant_size_mb = os.path.getsize('student_quantized.pth') / (1024**2)\n",
        "\n",
        "print(f\"\\nQuantized Student Results:\")\n",
        "print(f\"   Accuracy: {student_quant_accuracy:.2f}% (drop: {student_accuracy - student_quant_accuracy:.2f}%)\")\n",
        "print(f\"   Latency (CPU): {student_quant_latency:.2f} ms ({student_latency/student_quant_latency:.1f}x faster)\")\n",
        "print(f\"   Size: {quant_size_mb:.2f} MB ({student_stats['size_mb']/quant_size_mb:.1f}x smaller)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tjdIm4lXp-J"
      },
      "source": [
        "Export TFLite and ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6eiV674XpGQ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXPORT TO DEPLOYMENT FORMATS\n",
        "# ============================================================================\n",
        "print(\"\\n Exporting models for deployment...\")\n",
        "\n",
        "# 1. Export to ONNX\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "student.eval()\n",
        "\n",
        "torch.onnx.export(\n",
        "    student,\n",
        "    dummy_input,\n",
        "    \"student_model.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        ")\n",
        "\n",
        "print(\"Exported to ONNX: student_model.onnx\")\n",
        "\n",
        "# Verify ONNX model\n",
        "onnx_model = onnx.load(\"student_model.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"   ONNX model verified\")\n",
        "\n",
        "# 2. Export to TorchScript (for mobile)\n",
        "scripted_model = torch.jit.script(student.cpu())\n",
        "scripted_model.save(\"student_model.pt\")\n",
        "print(\"Exported to TorchScript: student_model.pt\")\n",
        "\n",
        "# 3. Benchmark ONNX Runtime\n",
        "ort_session = onnxruntime.InferenceSession(\"student_model.onnx\")\n",
        "\n",
        "def benchmark_onnx(session, num_runs=100):\n",
        "    dummy = np.random.randn(1, 3, 32, 32).astype(np.float32)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = session.run(None, {'input': dummy})\n",
        "\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        _ = session.run(None, {'input': dummy})\n",
        "    end = time.time()\n",
        "\n",
        "    return (end - start) / num_runs * 1000\n",
        "\n",
        "onnx_latency = benchmark_onnx(ort_session)\n",
        "print(f\"   ONNX Runtime latency: {onnx_latency:.2f} ms\")\n",
        "\n",
        "print(\"\\nExported Files:\")\n",
        "print(\"    student_model.onnx (for mobile/edge)\")\n",
        "print(\"    student_model.pt (TorchScript)\")\n",
        "print(\"    student_quantized.pth (quantized weights)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHe9mDvIX4G1"
      },
      "source": [
        "Comprehensive Bechmarking Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGBlO0SdX9qt"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RESULTS COMPILATION\n",
        "# ============================================================================\n",
        "\n",
        "results = {\n",
        "    'Model': ['Teacher (ResNet18)', 'Student (Distilled)', 'Student (Quantized)', 'ONNX Runtime'],\n",
        "    'Accuracy (%)': [teacher_accuracy, student_accuracy, student_quant_accuracy, student_accuracy],\n",
        "    'Latency (ms)': [teacher_latency, student_latency, student_quant_latency, onnx_latency],\n",
        "    'Parameters (M)': [\n",
        "        baseline_stats['params'] / 1e6,\n",
        "        student_stats['params'] / 1e6,\n",
        "        student_stats['params'] / 1e6,\n",
        "        student_stats['params'] / 1e6\n",
        "    ],\n",
        "    'Size (MB)': [baseline_stats['size_mb'], student_stats['size_mb'], quant_size_mb, student_stats['size_mb']],\n",
        "    'FLOPs (M)': [\n",
        "        baseline_stats['flops'] / 1e6,\n",
        "        student_stats['flops'] / 1e6,\n",
        "        student_stats['flops'] / 1e6,\n",
        "        student_stats['flops'] / 1e6\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Calculate improvements\n",
        "df['Speedup'] = df['Latency (ms)'].iloc[0] / df['Latency (ms)']\n",
        "df['Size Reduction'] = df['Size (MB)'].iloc[0] / df['Size (MB)']\n",
        "df['Accuracy Drop'] = df['Accuracy (%)'].iloc[0] - df['Accuracy (%)']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OPTIMIZATION RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION DASHBOARD\n",
        "# ============================================================================\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('Accuracy Comparison', 'Latency Comparison',\n",
        "                    'Model Size Comparison', 'Speedup vs Accuracy Trade-off'),\n",
        "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
        "           [{'type': 'bar'}, {'type': 'scatter'}]]\n",
        ")\n",
        "\n",
        "# Accuracy\n",
        "fig.add_trace(\n",
        "    go.Bar(x=df['Model'], y=df['Accuracy (%)'], name='Accuracy',\n",
        "           marker_color='lightblue', text=df['Accuracy (%)'].round(2),\n",
        "           textposition='outside'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Latency\n",
        "fig.add_trace(\n",
        "    go.Bar(x=df['Model'], y=df['Latency (ms)'], name='Latency',\n",
        "           marker_color='lightcoral', text=df['Latency (ms)'].round(2),\n",
        "           textposition='outside'),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Size\n",
        "fig.add_trace(\n",
        "    go.Bar(x=df['Model'], y=df['Size (MB)'], name='Size',\n",
        "           marker_color='lightgreen', text=df['Size (MB)'].round(2),\n",
        "           textposition='outside'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Pareto frontier\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=df['Accuracy Drop'], y=df['Speedup'], mode='markers+text',\n",
        "               name='Models', marker=dict(size=15, color=df.index),\n",
        "               text=df['Model'], textposition='top center'),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text=\"Model\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Model\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Model\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Accuracy Drop (%)\", row=2, col=2)\n",
        "\n",
        "fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Latency (ms)\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Size (MB)\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Speedup (x)\", row=2, col=2)\n",
        "\n",
        "fig.update_layout(height=800, title_text=\"Edge ML Optimization Dashboard\", showlegend=False)\n",
        "fig.show()\n",
        "\n",
        "# Save results\n",
        "df.to_csv('optimization_results.csv', index=False)\n",
        "print(\"\\n Results saved to: optimization_results.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODqTkMK7O+UqLv/+JIj28Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}